"""
üöÄ FASTER WHISPER SERVICE - OTIMIZADO PARA M√ÅXIMA VELOCIDADE
Implementa transcri√ß√£o com faster-whisper (at√© 4x mais r√°pido)
"""

import logging
import numpy as np
import torch
from typing import Optional, Dict, Any, List
import time
import psutil
import os
from pathlib import Path

logger = logging.getLogger(__name__)

try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False
    logger.warning("‚ö†Ô∏è faster-whisper n√£o dispon√≠vel. Instale com: pip install faster-whisper")

from app.services.progress_service import progress_service


class FasterWhisperService:
    """
    ‚úÖ SERVI√áO OTIMIZADO DE TRANSCRI√á√ÉO COM FASTER-WHISPER
    
    Benef√≠cios:
    - 4x mais r√°pido que whisper original
    - Menor uso de mem√≥ria
    - Suporte a quantiza√ß√£o int8
    - Processamento em lote otimizado
    """
    
    def __init__(self):
        logger.info("üöÄ Inicializando FasterWhisperService ULTRA OTIMIZADO")
        self._model = None
        self._device = self._detect_optimal_device()
        self._compute_type = self._detect_optimal_compute_type()
        self._model_size = self._select_optimal_model_size()
        
        logger.info(f"üñ•Ô∏è Device: {self._device}")
        logger.info(f"‚öôÔ∏è Compute type: {self._compute_type}")
        logger.info(f"üì¶ Model size: {self._model_size}")
    
    def _detect_optimal_device(self) -> str:
        """Detecta o melhor device para usar"""
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"üíæ GPU Memory: {gpu_memory:.1f}GB")
            
            if gpu_memory >= 4.0:
                return "cuda"
            else:
                logger.info("‚ö†Ô∏è GPU com pouca mem√≥ria, usando CPU")
                return "cpu"
        else:
            return "cpu"
    
    def _detect_optimal_compute_type(self) -> str:
        """Detecta o melhor tipo de computa√ß√£o"""
        if self._device == "cuda":
            # GPU: usar float16 para velocidade
            return "float16"
        else:
            # CPU: usar int8 para efici√™ncia m√°xima
            cpu_count = psutil.cpu_count()
            memory_gb = psutil.virtual_memory().total / 1024**3
            
            logger.info(f"üíª CPU cores: {cpu_count}, RAM: {memory_gb:.1f}GB")
            
            if memory_gb >= 8.0:
                return "int8"  # Quantiza√ß√£o para velocidade
            else:
                return "float32"  # Fallback para compatibilidade
    
    def _select_optimal_model_size(self) -> str:
        """Seleciona o tamanho √≥timo do modelo baseado no hardware"""
        if self._device == "cuda":
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            if gpu_memory >= 8.0:
                return "large-v3"    # GPU potente
            elif gpu_memory >= 4.0:
                return "medium"      # GPU m√©dia
            else:
                return "small"       # GPU limitada
        else:
            memory_gb = psutil.virtual_memory().total / 1024**3
            cpu_count = psutil.cpu_count()
            
            if memory_gb >= 16.0 and cpu_count >= 8:
                return "medium"      # CPU potente
            elif memory_gb >= 8.0:
                return "small"       # CPU m√©dia
            else:
                return "base"        # CPU limitada
    
    @property
    def model(self) -> "WhisperModel":
        """Carrega o modelo faster-whisper com lazy loading"""
        if not FASTER_WHISPER_AVAILABLE:
            raise RuntimeError("faster-whisper n√£o est√° dispon√≠vel. Instale com: pip install faster-whisper")
        
        if self._model is None:
            logger.info(f"üîÑ Carregando faster-whisper modelo: {self._model_size}")
            start_time = time.time()
            
            try:
                self._model = WhisperModel(
                    self._model_size,
                    device=self._device,
                    compute_type=self._compute_type,
                    cpu_threads=min(8, psutil.cpu_count()) if self._device == "cpu" else 0,
                    num_workers=1  # Otimizado para single audio
                )
                
                load_time = time.time() - start_time
                logger.info(f"‚úÖ Faster-whisper carregado em {load_time:.2f}s")
                
            except Exception as e:
                logger.error(f"‚ùå Erro ao carregar faster-whisper: {e}")
                # Fallback para modelo menor
                logger.info("üîÑ Tentando modelo menor como fallback...")
                self._model_size = "base"
                self._model = WhisperModel(
                    self._model_size,
                    device=self._device,
                    compute_type="float32"  # Mais compat√≠vel
                )
                logger.info("‚úÖ Faster-whisper carregado com modelo base")
        
        return self._model
    
    def transcribe_audio_optimized(
        self, 
        audio_data: np.ndarray, 
        sample_rate: int, 
        task_id: Optional[str] = None
    ) -> str:
        """
        ‚úÖ TRANSCRI√á√ÉO ULTRA OTIMIZADA COM FASTER-WHISPER
        """
        try:
            logger.info("‚ö° Iniciando transcri√ß√£o ULTRA OTIMIZADA com faster-whisper")
            start_time = time.time()
            
            # Atualiza progresso
            if task_id:
                progress_service.update_progress(task_id, "audio_processing", 60)
            
            # Normaliza √°udio para faster-whisper
            if audio_data.dtype != np.float32:
                audio_data = audio_data.astype(np.float32)
            
            # Normaliza amplitude
            if np.max(np.abs(audio_data)) > 1.0:
                audio_data = audio_data / np.max(np.abs(audio_data))
            
            duration = len(audio_data) / sample_rate
            logger.info(f"üîä √Åudio processado: {duration:.2f}s, {sample_rate}Hz")
            
            # Configura√ß√µes otimizadas para faster-whisper
            transcribe_options = {
                "language": "pt",
                "task": "transcribe",
                "beam_size": 1 if duration <= 30 else 2,  # Adaptativo
                "best_of": 1,  # Greedy para velocidade m√°xima
                "temperature": 0.0,  # Determin√≠stico
                "compression_ratio_threshold": 2.4,
                "log_prob_threshold": -1.0,
                "no_speech_threshold": 0.6,
                "condition_on_previous_text": False,  # Mais r√°pido
                "word_timestamps": False,  # Desabilita para velocidade
                "vad_filter": True,  # Voice Activity Detection
                "vad_parameters": {
                    "threshold": 0.5,
                    "min_speech_duration_ms": 250,
                    "max_speech_duration_s": 30,
                    "min_silence_duration_ms": 100
                }
            }
            
            logger.info(f"üéØ Configura√ß√µes faster-whisper: beam_size={transcribe_options['beam_size']}")
            
            # Atualiza progresso
            if task_id:
                progress_service.update_progress(task_id, "transcription_processing", 80)
            
            # Executa transcri√ß√£o
            model = self.model
            segments, info = model.transcribe(
                audio_data,
                **transcribe_options
            )
            
            # Processa segmentos
            transcription_parts = []
            for segment in segments:
                text = segment.text.strip()
                if text:
                    transcription_parts.append(text)
            
            # Junta resultado
            if transcription_parts:
                transcription_text = " ".join(transcription_parts)
            else:
                transcription_text = "[√Åudio sem fala detectada]"
            
            # P√≥s-processamento PT-BR
            transcription_text = self._postprocess_portuguese_text(transcription_text)
            
            # Estat√≠sticas de performance
            total_time = time.time() - start_time
            speed_ratio = duration / total_time if total_time > 0 else 0
            
            logger.info(f"‚úÖ Faster-whisper CONCLU√çDO:")
            logger.info(f"   üìä Tempo: {total_time:.2f}s")
            logger.info(f"   üöÄ Velocidade: {speed_ratio:.1f}x tempo real")
            logger.info(f"   üìù Caracteres: {len(transcription_text)}")
            logger.info(f"   üéØ Modelo: {self._model_size} ({self._compute_type})")
            
            # Atualiza progresso final
            if task_id:
                progress_service.update_progress(task_id, "transcription_processing", 100)
            
            return transcription_text
            
        except Exception as e:
            logger.error(f"‚ùå Erro na transcri√ß√£o faster-whisper: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise RuntimeError(f"Erro na transcri√ß√£o faster-whisper: {str(e)}")
    
    def _postprocess_portuguese_text(self, text: str) -> str:
        """P√≥s-processamento espec√≠fico para portugu√™s brasileiro"""
        if not text or text.strip() == "":
            return ""
        
        # Remove prefixos comuns do Whisper
        prefixes_to_remove = [
            "Transcri√ß√£o:",
            "Texto:",
            "Audio:",
            "√Åudio:",
            "[M√öSICA]",
            "[MUSIC]",
            "[M√∫sica]"
        ]
        
        for prefix in prefixes_to_remove:
            if text.startswith(prefix):
                text = text[len(prefix):].strip()
        
        # Capitaliza primeira letra
        if text:
            text = text[0].upper() + text[1:] if len(text) > 1 else text.upper()
        
        # Remove espa√ßos m√∫ltiplos
        import re
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas de performance do sistema"""
        stats = {
            "device": self._device,
            "compute_type": self._compute_type,
            "model_size": self._model_size,
            "faster_whisper_available": FASTER_WHISPER_AVAILABLE
        }
        
        if self._device == "cuda" and torch.cuda.is_available():
            stats.update({
                "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory / 1024**3,
                "gpu_memory_allocated": torch.cuda.memory_allocated() / 1024**3,
                "gpu_name": torch.cuda.get_device_name(0)
            })
        
        stats.update({
            "cpu_count": psutil.cpu_count(),
            "memory_total": psutil.virtual_memory().total / 1024**3,
            "memory_available": psutil.virtual_memory().available / 1024**3
        })
        
        return stats


# Inst√¢ncia global do servi√ßo
faster_whisper_service = FasterWhisperService() 