import json
import re
import time
import logging
from typing import List, Dict, Optional, Tuple, Set
from collections import Counter, defaultdict

from app.schemas.transcription import (
    MeetingAnalysisResult, ParticipantInfo, TopicInfo, ActionItem, 
    KeyDecision, SentimentAnalysis, Priority
)

logger = logging.getLogger(__name__)


class MeetingAnalysisService:
    """
    Servi√ßo de an√°lise inteligente de reuni√µes.
    Extrai participantes, t√≥picos, tarefas e decis√µes do texto transcrito.
    """
    
    def __init__(self):
        # Padr√µes para identifica√ß√£o de participantes
        self.participant_patterns = [
            r'\b([A-Z][a-z√°√™√ß√µ√º]+(?:\s+[A-Z][a-z√°√™√ß√µ√º]+)*)\s+(?:disse|falou|comentou|mencionou|perguntou|respondeu)',
            r'(?:como\s+(?:disse|falou)\s+(?:o|a)?\s*)([A-Z][a-z√°√™√ß√µ√º]+(?:\s+[A-Z][a-z√°√™√ß√µ√º]+)*)',
            r'(?:segundo\s+(?:o|a)?\s*)([A-Z][a-z√°√™√ß√µ√º]+(?:\s+[A-Z][a-z√°√™√ß√µ√º]+)*)',
            r'([A-Z][a-z√°√™√ß√µ√º]+(?:\s+[A-Z][a-z√°√™√ß√µ√º]+)*)\s+(?:explicou|apresentou|prop√¥s|sugeriu)',
            r'(?:meu\s+nome\s+√©|eu\s+sou\s+(?:o|a)?\s*)([A-Z][a-z√°√™√ß√µ√º]+(?:\s+[A-Z][a-z√°√™√ß√µ√º]+)*)',
        ]
        
        # Padr√µes para identifica√ß√£o de tarefas/a√ß√µes
        self.action_patterns = [
            r'(?:precisa|deve|vai|tem\s+que|fica\s+respons√°vel\s+por|encarregado\s+de)\s+([^.!?]+)',
            r'([A-Z][a-z√°√™√ß√µ√º]+(?:\s+[A-Z][a-z√°√™√ß√µ√º]+)*)\s+(?:vai|deve|precisa|fica\s+respons√°vel)\s+([^.!?]+)',
            r'(?:vamos|devemos|precisamos)\s+([^.!?]+)',
            r'(?:a√ß√£o|tarefa|atividade):\s*([^.!?]+)',
            r'(?:entregar|enviar|preparar|revisar|fazer|executar|implementar|desenvolver)\s+([^.!?]+)',
        ]
        
        # Padr√µes para identifica√ß√£o de prazos
        self.deadline_patterns = [
            r'(?:at√©|antes\s+de|prazo\s+de?|deadline)\s+([^.!?]+)',
            r'(?:pr√≥xima\s+(?:semana|segunda|ter√ßa|quarta|quinta|sexta))',
            r'(?:amanh√£|hoje|ontem)',
            r'(?:\d{1,2}\s+de\s+\w+|\d{1,2}/\d{1,2})',
            r'(?:final\s+do\s+m√™s|in√≠cio\s+do\s+m√™s)',
        ]
        
        # Padr√µes para identifica√ß√£o de decis√µes
        self.decision_patterns = [
            r'(?:decidiu-se|ficou\s+decidido|foi\s+aprovado|concordamos)\s+([^.!?]+)',
            r'(?:decis√£o|resolu√ß√£o|acordo):\s*([^.!?]+)',
            r'(?:vamos\s+adotar|optamos\s+por|escolhemos)\s+([^.!?]+)',
        ]
        
        # Palavras-chave para t√≥picos importantes
        self.topic_keywords = [
            'projeto', 'or√ßamento', 'estrat√©gia', 'plano', 'meta', 'objetivo',
            'problema', 'solu√ß√£o', 'proposta', 'apresenta√ß√£o', 'relat√≥rio',
            'cliente', 'parceiro', 'fornecedor', 'equipe', 'time', 'departamento',
            'produto', 'servi√ßo', 'vendas', 'marketing', 'financeiro',
            'cronograma', 'prazo', 'entrega', 'milestone', 'fase'
        ]
        
        # Palavras para an√°lise de sentimento
        self.positive_words = [
            'excelente', '√≥timo', 'bom', 'positivo', 'sucesso', 'aprovado',
            'concordo', 'perfeito', 'satisfeito', 'feliz', 'animado'
        ]
        
        self.negative_words = [
            'problema', 'dificuldade', 'ruim', 'negativo', 'fracasso', 'rejeitado',
            'discordo', 'preocupado', 'insatisfeito', 'frustrado', 'atrasado'
        ]
        
        # Nomes brasileiros comuns para filtrar falsos positivos
        self.common_names = {
            'jo√£o', 'maria', 'jos√©', 'ana', 'antonio', 'francisca', 'carlos', 'paulo',
            'pedro', 'lucas', 'luiz', 'marcos', 'luis', 'gabriel', 'rafael', 'daniel',
            'marcelo', 'bruno', 'eduardo', 'felipe', 'rodrigo', 'manoel', 'ricardo',
            'adriana', 'juliana', 'patricia', 'sandra', 'monica', 'claudia', 'fernanda',
            'carla', 'cristina', 'rosana', 'luciana', 'marcia', 'andrea', 'leticia'
        }

    async def analyze_meeting(
        self, 
        transcription_text: str, 
        include_sentiment: bool = True,
        extract_participants: bool = True,
        extract_action_items: bool = True,
        min_confidence: float = 0.6
    ) -> MeetingAnalysisResult:
        """
        Realiza an√°lise completa da reuni√£o extraindo todas as informa√ß√µes relevantes.
        """
        start_time = time.time()
        logger.info("üîç Iniciando an√°lise inteligente da reuni√£o")
        
        try:
            # Pr√©-processamento do texto
            cleaned_text = self._preprocess_text(transcription_text)
            
            # Extra√ß√£o de participantes
            participants = []
            if extract_participants:
                participants = self._extract_participants(cleaned_text, min_confidence)
                logger.info(f"‚úÖ Participantes extra√≠dos: {len(participants)}")
            
            # Extra√ß√£o de t√≥picos principais
            main_topics = self._extract_main_topics(cleaned_text, min_confidence)
            logger.info(f"‚úÖ T√≥picos extra√≠dos: {len(main_topics)}")
            
            # Extra√ß√£o de itens de a√ß√£o
            action_items = []
            if extract_action_items:
                action_items = self._extract_action_items(cleaned_text, participants, min_confidence)
                logger.info(f"‚úÖ Itens de a√ß√£o extra√≠dos: {len(action_items)}")
            
            # Extra√ß√£o de decis√µes importantes
            key_decisions = self._extract_key_decisions(cleaned_text, min_confidence)
            logger.info(f"‚úÖ Decis√µes extra√≠das: {len(key_decisions)}")
            
            # Gera√ß√£o de resumo estruturado
            summary = self._generate_structured_summary(
                cleaned_text, participants, main_topics, action_items, key_decisions
            )
            
            # An√°lise de sentimento
            sentiment_analysis = None
            if include_sentiment:
                sentiment_analysis = self._analyze_sentiment(cleaned_text, main_topics)
                logger.info("‚úÖ An√°lise de sentimento conclu√≠da")
            
            # C√°lculo da confian√ßa geral
            confidence_score = self._calculate_overall_confidence(
                participants, main_topics, action_items, key_decisions
            )
            
            processing_time = time.time() - start_time
            
            result = MeetingAnalysisResult(
                participants=participants,
                main_topics=main_topics,
                action_items=action_items,
                key_decisions=key_decisions,
                summary=summary,
                sentiment_analysis=sentiment_analysis,
                confidence_score=confidence_score,
                processing_time=processing_time
            )
            
            logger.info(f"üéØ An√°lise conclu√≠da em {processing_time:.2f}s com confian√ßa {confidence_score:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise da reuni√£o: {str(e)}")
            raise
    
    def _preprocess_text(self, text: str) -> str:
        """Pr√©-processa o texto removendo ru√≠dos e normalizando."""
        # Remove caracteres especiais desnecess√°rios
        text = re.sub(r'[^\w\s\.\!\?\,\:\;\-\(\)]', ' ', text)
        
        # Normaliza espa√ßos m√∫ltiplos
        text = re.sub(r'\s+', ' ', text)
        
        # Remove linhas muito curtas (provavelmente ru√≠do)
        lines = text.split('\n')
        cleaned_lines = [line.strip() for line in lines if len(line.strip()) > 10]
        
        return ' '.join(cleaned_lines)
    
    def _extract_participants(self, text: str, min_confidence: float) -> List[ParticipantInfo]:
        """Extrai participantes da reuni√£o usando padr√µes de NLP."""
        logger.info("üë• Extraindo participantes da reuni√£o")
        
        participant_mentions = defaultdict(int)
        participant_contexts = defaultdict(list)
        
        # Busca por padr√µes de men√ß√£o a participantes
        for pattern in self.participant_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                name = match.group(1).strip()
                
                # Filtra nomes muito curtos ou comuns demais
                if len(name) < 3 or name.lower() in ['eu', 'ele', 'ela', 'voc√™', 'n√≥s']:
                    continue
                
                # Normaliza o nome
                name = self._normalize_name(name)
                
                if name:
                    participant_mentions[name] += 1
                    # Captura contexto ao redor da men√ß√£o
                    start = max(0, match.start() - 50)
                    end = min(len(text), match.end() + 50)
                    context = text[start:end].strip()
                    participant_contexts[name].append(context)
        
        # Converte para lista de ParticipantInfo
        participants = []
        for name, mentions in participant_mentions.items():
            if mentions >= 1:  # Pelo menos uma men√ß√£o
                # Tenta identificar fun√ß√£o/cargo
                role = self._extract_participant_role(participant_contexts[name])
                
                # Calcula confian√ßa baseada no n√∫mero de men√ß√µes e contexto
                confidence = min(0.9, 0.5 + (mentions * 0.1) + (0.2 if role else 0))
                
                if confidence >= min_confidence:
                    participants.append(ParticipantInfo(
                        name=name,
                        mentions=mentions,
                        role=role,
                        confidence=confidence
                    ))
        
        # Ordena por n√∫mero de men√ß√µes
        participants.sort(key=lambda p: p.mentions, reverse=True)
        
        logger.info(f"‚úÖ {len(participants)} participantes identificados")
        return participants[:10]  # M√°ximo 10 participantes
    
    def _normalize_name(self, name: str) -> Optional[str]:
        """Normaliza e valida nomes de participantes."""
        # Remove artigos e preposi√ß√µes
        words = name.split()
        filtered_words = []
        
        for word in words:
            word_clean = word.lower().strip('.,!?:;')
            if word_clean not in ['o', 'a', 'de', 'da', 'do', 'dos', 'das', 'e']:
                if len(word_clean) >= 2 and word_clean.isalpha():
                    filtered_words.append(word.capitalize())
        
        if len(filtered_words) >= 1:
            normalized = ' '.join(filtered_words)
            # Verifica se √© um nome v√°lido (n√£o apenas palavras comuns)
            first_name = filtered_words[0].lower()
            if first_name in self.common_names or len(filtered_words) >= 2:
                return normalized
        
        return None
    
    def _extract_participant_role(self, contexts: List[str]) -> Optional[str]:
        """Extrai fun√ß√£o/cargo do participante baseado no contexto."""
        role_patterns = [
            r'(?:diretor|gerente|coordenador|supervisor|analista|assistente|estagi√°rio)',
            r'(?:CEO|CTO|CFO|CMO|VP|presidente|vice)',
            r'(?:desenvolvedor|programador|designer|arquiteto|consultor)',
            r'(?:vendedor|comercial|marketing|financeiro|RH|recursos\s+humanos)'
        ]
        
        all_context = ' '.join(contexts).lower()
        
        for pattern in role_patterns:
            match = re.search(pattern, all_context, re.IGNORECASE)
            if match:
                return match.group().title()
        
        return None
    
    def _extract_main_topics(self, text: str, min_confidence: float) -> List[TopicInfo]:
        """Extrai t√≥picos principais da reuni√£o."""
        logger.info("üìã Extraindo t√≥picos principais")
        
        # Divide o texto em segmentos/par√°grafos
        segments = self._segment_text_by_topics(text)
        
        topics = []
        for i, segment in enumerate(segments):
            if len(segment.strip()) < 50:  # Segmento muito pequeno
                continue
            
            # Identifica o t√≥pico principal do segmento
            topic_title = self._extract_topic_title(segment)
            if not topic_title:
                continue
            
            # Gera resumo do segmento
            topic_summary = self._summarize_segment(segment)
            
            # Extrai palavras-chave
            keywords = self._extract_keywords(segment)
            
            # Calcula import√¢ncia baseada em palavras-chave e tamanho
            importance = self._calculate_topic_importance(segment, keywords)
            
            if importance >= 0.3:  # Filtro de relev√¢ncia
                topics.append(TopicInfo(
                    title=topic_title,
                    summary=topic_summary,
                    keywords=keywords,
                    importance=importance,
                    duration_mentioned=f"~{len(segment.split())//20}min"  # Estimativa grosseira
                ))
        
        # Ordena por import√¢ncia
        topics.sort(key=lambda t: t.importance, reverse=True)
        
        logger.info(f"‚úÖ {len(topics)} t√≥picos principais identificados")
        return topics[:5]  # M√°ximo 5 t√≥picos principais
    
    def _segment_text_by_topics(self, text: str) -> List[str]:
        """Segmenta o texto em blocos por t√≥picos."""
        # Estrat√©gia simples: divide por senten√ßas e agrupa por similaridade sem√¢ntica
        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if len(s.strip()) > 20]
        
        # Agrupa senten√ßas em segmentos de ~3-5 senten√ßas
        segments = []
        current_segment = []
        
        for sentence in sentences:
            current_segment.append(sentence)
            
            # Cria novo segmento a cada 4 senten√ßas ou quando detecta mudan√ßa de t√≥pico
            if len(current_segment) >= 4 or self._detects_topic_change(sentence):
                if current_segment:
                    segments.append('. '.join(current_segment) + '.')
                    current_segment = []
        
        # Adiciona √∫ltimo segmento se houver
        if current_segment:
            segments.append('. '.join(current_segment) + '.')
        
        return segments
    
    def _detects_topic_change(self, sentence: str) -> bool:
        """Detecta se uma senten√ßa indica mudan√ßa de t√≥pico."""
        topic_change_indicators = [
            'agora vamos', 'pr√≥ximo ponto', 'pr√≥ximo item', 'mudando de assunto',
            'falando sobre', 'sobre o tema', 'outro t√≥pico', 'outra quest√£o'
        ]
        
        sentence_lower = sentence.lower()
        return any(indicator in sentence_lower for indicator in topic_change_indicators)
    
    def _extract_topic_title(self, segment: str) -> Optional[str]:
        """Extrai t√≠tulo do t√≥pico de um segmento."""
        # Busca por frases que indiquem o t√≥pico
        topic_indicators = [
            r'(?:sobre|falando\s+sobre|discutindo|apresentando)\s+([^.!?]{10,50})',
            r'(?:projeto|plano|estrat√©gia|proposta)\s+([^.!?]{5,30})',
            r'(?:quest√£o|problema|assunto)\s+(?:da?|do)\s+([^.!?]{5,30})'
        ]
        
        for pattern in topic_indicators:
            match = re.search(pattern, segment, re.IGNORECASE)
            if match:
                title = match.group(1).strip()
                return title.capitalize()
        
        # Fallback: usa as primeiras palavras importantes
        words = segment.split()[:10]
        important_words = [w for w in words if w.lower() in self.topic_keywords]
        
        if important_words:
            return ' '.join(important_words[:3]).capitalize()
        
        # √öltimo recurso: primeiras palavras do segmento
        first_sentence = segment.split('.')[0]
        if len(first_sentence) > 10 and len(first_sentence) < 60:
            return first_sentence.strip().capitalize()
        
        return None
    
    def _summarize_segment(self, segment: str) -> str:
        """Gera resumo de um segmento de texto."""
        sentences = [s.strip() for s in segment.split('.') if len(s.strip()) > 15]
        
        if len(sentences) <= 2:
            return segment.strip()
        
        # Seleciona as 2 senten√ßas mais importantes
        sentence_scores = []
        for sentence in sentences:
            score = 0
            sentence_lower = sentence.lower()
            
            # Pontua por palavras-chave
            for keyword in self.topic_keywords:
                if keyword in sentence_lower:
                    score += 1
            
            # Pontua por verbos de a√ß√£o
            action_verbs = ['decidiu', 'definiu', 'prop√¥s', 'apresentou', 'discutiu']
            for verb in action_verbs:
                if verb in sentence_lower:
                    score += 1
            
            sentence_scores.append((sentence, score))
        
        # Seleciona as melhores senten√ßas
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        top_sentences = [s[0] for s in sentence_scores[:2]]
        
        return '. '.join(top_sentences) + '.'
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extrai palavras-chave de um texto."""
        words = re.findall(r'\b\w+\b', text.lower())
        
        # Filtra palavras relevantes
        relevant_words = []
        for word in words:
            if (len(word) >= 4 and 
                word in self.topic_keywords and 
                word not in ['para', 'como', 'sobre', 'mais', 'muito', 'bem']):
                relevant_words.append(word)
        
        # Conta frequ√™ncia e retorna as mais comuns
        word_counts = Counter(relevant_words)
        return [word for word, count in word_counts.most_common(5)]
    
    def _calculate_topic_importance(self, segment: str, keywords: List[str]) -> float:
        """Calcula a import√¢ncia de um t√≥pico."""
        importance = 0.0
        
        # Baseado no n√∫mero de palavras-chave
        importance += len(keywords) * 0.1
        
        # Baseado no tamanho do segmento
        word_count = len(segment.split())
        if 50 <= word_count <= 200:
            importance += 0.3
        elif word_count > 200:
            importance += 0.2
        
        # Baseado na presen√ßa de verbos de a√ß√£o/decis√£o
        action_words = ['decidiu', 'definiu', 'acordou', 'prop√¥s', 'apresentou']
        for word in action_words:
            if word in segment.lower():
                importance += 0.2
                break
        
        return min(1.0, importance)
    
    def _extract_action_items(self, text: str, participants: List[ParticipantInfo], min_confidence: float) -> List[ActionItem]:
        """Extrai itens de a√ß√£o/tarefas da reuni√£o."""
        logger.info("üìù Extraindo itens de a√ß√£o")
        
        action_items = []
        participant_names = [p.name.lower() for p in participants]
        
        for pattern in self.action_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                task_text = match.group(1).strip() if len(match.groups()) >= 1 else match.group().strip()
                
                # Limpa e valida a tarefa
                task = self._clean_task_text(task_text)
                if not task or len(task) < 10:
                    continue
                
                # Tenta identificar respons√°vel
                assignee = self._extract_assignee(match.group(), participant_names)
                
                # Tenta identificar prazo
                due_date = self._extract_due_date(text, match.start(), match.end())
                
                # Determina prioridade
                priority = self._determine_priority(task, match.group())
                
                # Calcula confian√ßa
                confidence = self._calculate_action_confidence(task, assignee, due_date)
                
                if confidence >= min_confidence:
                    action_items.append(ActionItem(
                        task=task,
                        assignee=assignee,
                        due_date=due_date,
                        priority=priority,
                        confidence=confidence
                    ))
        
        # Remove duplicatas similares
        action_items = self._deduplicate_actions(action_items)
        
        logger.info(f"‚úÖ {len(action_items)} itens de a√ß√£o identificados")
        return action_items[:10]  # M√°ximo 10 a√ß√µes
    
    def _clean_task_text(self, task_text: str) -> Optional[str]:
        """Limpa e normaliza o texto da tarefa."""
        # Remove palavras desnecess√°rias do in√≠cio
        task = re.sub(r'^(que|de|para|com|por)\s+', '', task_text.strip(), flags=re.IGNORECASE)
        
        # Remove pontua√ß√£o final
        task = task.rstrip('.,!?:;')
        
        # Capitaliza primeira letra
        if task and task[0].islower():
            task = task[0].upper() + task[1:]
        
        return task if len(task) >= 10 else None
    
    def _extract_assignee(self, context: str, participant_names: List[str]) -> Optional[str]:
        """Extrai respons√°vel pela tarefa do contexto."""
        context_lower = context.lower()
        
        # Busca por nomes de participantes no contexto
        for name in participant_names:
            if name in context_lower:
                return name.title()
        
        # Busca por pronomes que indicam respons√°vel
        if re.search(r'\b(eu|vou|farei)\b', context_lower):
            return "Falante atual"
        
        return None
    
    def _extract_due_date(self, text: str, start_pos: int, end_pos: int) -> Optional[str]:
        """Extrai prazo mencionado pr√≥ximo √† tarefa."""
        # Busca em um contexto de 100 caracteres ao redor da tarefa
        context_start = max(0, start_pos - 100)
        context_end = min(len(text), end_pos + 100)
        context = text[context_start:context_end]
        
        for pattern in self.deadline_patterns:
            match = re.search(pattern, context, re.IGNORECASE)
            if match:
                return match.group().strip()
        
        return None
    
    def _determine_priority(self, task: str, context: str) -> Priority:
        """Determina prioridade da tarefa baseada no contexto."""
        high_priority_words = ['urgente', 'imediato', 'cr√≠tico', 'importante', 'prioridade']
        low_priority_words = ['quando poss√≠vel', 'sem pressa', 'eventualmente']
        
        context_lower = (task + ' ' + context).lower()
        
        if any(word in context_lower for word in high_priority_words):
            return Priority.ALTA
        elif any(word in context_lower for word in low_priority_words):
            return Priority.BAIXA
        else:
            return Priority.MEDIA
    
    def _calculate_action_confidence(self, task: str, assignee: Optional[str], due_date: Optional[str]) -> float:
        """Calcula confian√ßa na extra√ß√£o de uma a√ß√£o."""
        confidence = 0.5  # Base
        
        # Aumenta confian√ßa se tem respons√°vel
        if assignee:
            confidence += 0.2
        
        # Aumenta confian√ßa se tem prazo
        if due_date:
            confidence += 0.2
        
        # Aumenta confian√ßa se tem verbos de a√ß√£o claros
        action_verbs = ['fazer', 'executar', 'entregar', 'enviar', 'preparar', 'revisar']
        if any(verb in task.lower() for verb in action_verbs):
            confidence += 0.1
        
        return min(0.95, confidence)
    
    def _deduplicate_actions(self, actions: List[ActionItem]) -> List[ActionItem]:
        """Remove a√ß√µes duplicadas ou muito similares."""
        unique_actions = []
        
        for action in actions:
            is_duplicate = False
            for existing in unique_actions:
                # Verifica similaridade simples baseada nas primeiras palavras
                action_words = set(action.task.lower().split()[:5])
                existing_words = set(existing.task.lower().split()[:5])
                
                # Se mais de 60% das palavras s√£o iguais, considera duplicata
                if len(action_words & existing_words) / len(action_words | existing_words) > 0.6:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_actions.append(action)
        
        return unique_actions
    
    def _extract_key_decisions(self, text: str, min_confidence: float) -> List[KeyDecision]:
        """Extrai decis√µes importantes da reuni√£o."""
        logger.info("‚öñÔ∏è Extraindo decis√µes importantes")
        
        decisions = []
        
        for pattern in self.decision_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                decision_text = match.group(1).strip() if len(match.groups()) >= 1 else match.group().strip()
                
                # Limpa o texto da decis√£o
                decision = self._clean_decision_text(decision_text)
                if not decision or len(decision) < 15:
                    continue
                
                # Extrai contexto
                context = self._extract_decision_context(text, match.start(), match.end())
                
                # Determina impacto
                impact = self._determine_decision_impact(decision, context)
                
                # Calcula confian√ßa
                confidence = self._calculate_decision_confidence(decision, context)
                
                if confidence >= min_confidence:
                    decisions.append(KeyDecision(
                        decision=decision,
                        context=context,
                        impact=impact,
                        confidence=confidence
                    ))
        
        # Remove duplicatas
        decisions = self._deduplicate_decisions(decisions)
        
        logger.info(f"‚úÖ {len(decisions)} decis√µes importantes identificadas")
        return decisions[:5]  # M√°ximo 5 decis√µes
    
    def _clean_decision_text(self, decision_text: str) -> Optional[str]:
        """Limpa e normaliza o texto da decis√£o."""
        decision = decision_text.strip().rstrip('.,!?:;')
        
        if decision and decision[0].islower():
            decision = decision[0].upper() + decision[1:]
        
        return decision if len(decision) >= 15 else None
    
    def _extract_decision_context(self, text: str, start_pos: int, end_pos: int) -> str:
        """Extrai contexto ao redor de uma decis√£o."""
        context_start = max(0, start_pos - 150)
        context_end = min(len(text), end_pos + 150)
        context = text[context_start:context_end].strip()
        
        # Pega a senten√ßa completa
        sentences = re.split(r'[.!?]+', context)
        if len(sentences) >= 3:
            return sentences[len(sentences)//2].strip()
        
        return context[:200] + "..." if len(context) > 200 else context
    
    def _determine_decision_impact(self, decision: str, context: str) -> str:
        """Determina o impacto de uma decis√£o."""
        high_impact_words = ['estrat√©gico', 'importante', 'cr√≠tico', 'fundamental', 'grande']
        low_impact_words = ['pequeno', 'simples', 'menor', 'b√°sico']
        
        combined_text = (decision + ' ' + context).lower()
        
        if any(word in combined_text for word in high_impact_words):
            return 'alta'
        elif any(word in combined_text for word in low_impact_words):
            return 'baixa'
        else:
            return 'm√©dia'
    
    def _calculate_decision_confidence(self, decision: str, context: str) -> float:
        """Calcula confian√ßa na extra√ß√£o de uma decis√£o."""
        confidence = 0.6  # Base
        
        # Aumenta se tem palavras de decis√£o claras
        decision_words = ['decidiu', 'aprovado', 'definido', 'acordado', 'resolvido']
        if any(word in (decision + context).lower() for word in decision_words):
            confidence += 0.2
        
        # Aumenta se tem contexto substancial
        if len(context) > 50:
            confidence += 0.1
        
        return min(0.9, confidence)
    
    def _deduplicate_decisions(self, decisions: List[KeyDecision]) -> List[KeyDecision]:
        """Remove decis√µes duplicadas."""
        unique_decisions = []
        
        for decision in decisions:
            is_duplicate = False
            for existing in unique_decisions:
                # Verifica similaridade
                decision_words = set(decision.decision.lower().split()[:7])
                existing_words = set(existing.decision.lower().split()[:7])
                
                if len(decision_words & existing_words) / len(decision_words | existing_words) > 0.7:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_decisions.append(decision)
        
        return unique_decisions
    
    def _analyze_sentiment(self, text: str, topics: List[TopicInfo]) -> SentimentAnalysis:
        """Realiza an√°lise de sentimento do texto."""
        logger.info("üòä Analisando sentimento da reuni√£o")
        
        # An√°lise geral
        overall_sentiment = self._calculate_overall_sentiment(text)
        
        # An√°lise por t√≥pico
        topic_sentiments = {}
        for topic in topics:
            topic_sentiment = self._calculate_topic_sentiment(topic.summary)
            topic_sentiments[topic.title] = topic_sentiment
        
        return SentimentAnalysis(
            overall=overall_sentiment,
            topics=topic_sentiments,
            confidence=0.7  # An√°lise de sentimento simples tem confian√ßa moderada
        )
    
    def _calculate_overall_sentiment(self, text: str) -> str:
        """Calcula sentimento geral do texto."""
        text_lower = text.lower()
        
        positive_count = sum(1 for word in self.positive_words if word in text_lower)
        negative_count = sum(1 for word in self.negative_words if word in text_lower)
        
        if positive_count > negative_count * 1.5:
            return "positivo"
        elif negative_count > positive_count * 1.5:
            return "negativo"
        else:
            return "neutro"
    
    def _calculate_topic_sentiment(self, topic_text: str) -> str:
        """Calcula sentimento de um t√≥pico espec√≠fico."""
        return self._calculate_overall_sentiment(topic_text)
    
    def _generate_structured_summary(
        self, 
        text: str, 
        participants: List[ParticipantInfo], 
        topics: List[TopicInfo], 
        actions: List[ActionItem], 
        decisions: List[KeyDecision]
    ) -> str:
        """Gera resumo estruturado da reuni√£o."""
        summary_parts = []
        
        # Cabe√ßalho
        summary_parts.append("## RESUMO DA REUNI√ÉO")
        summary_parts.append("")
        
        # Participantes
        if participants:
            summary_parts.append("### üë• Participantes:")
            for p in participants[:5]:
                role_text = f" ({p.role})" if p.role else ""
                summary_parts.append(f"- {p.name}{role_text}")
            summary_parts.append("")
        
        # T√≥picos principais
        if topics:
            summary_parts.append("### üìã T√≥picos Principais:")
            for i, topic in enumerate(topics[:3], 1):
                summary_parts.append(f"{i}. **{topic.title}**")
                summary_parts.append(f"   {topic.summary}")
            summary_parts.append("")
        
        # Decis√µes importantes
        if decisions:
            summary_parts.append("### ‚öñÔ∏è Decis√µes Importantes:")
            for i, decision in enumerate(decisions[:3], 1):
                summary_parts.append(f"{i}. {decision.decision}")
            summary_parts.append("")
        
        # Itens de a√ß√£o
        if actions:
            summary_parts.append("### üìù Pr√≥ximas A√ß√µes:")
            for i, action in enumerate(actions[:5], 1):
                assignee_text = f" (Respons√°vel: {action.assignee})" if action.assignee else ""
                due_text = f" - Prazo: {action.due_date}" if action.due_date else ""
                priority_emoji = {"alta": "üî¥", "m√©dia": "üü°", "baixa": "üü¢"}.get(action.priority.value, "üü°")
                summary_parts.append(f"{i}. {priority_emoji} {action.task}{assignee_text}{due_text}")
            summary_parts.append("")
        
        # Resumo geral
        summary_parts.append("### üìÑ Resumo Geral:")
        general_summary = self._generate_general_summary(text)
        summary_parts.append(general_summary)
        
        return "\n".join(summary_parts)
    
    def _generate_general_summary(self, text: str) -> str:
        """Gera resumo geral do texto."""
        # Estrat√©gia simples: pega as 3 senten√ßas mais importantes
        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if len(s.strip()) > 20]
        
        if len(sentences) <= 3:
            return '. '.join(sentences) + '.'
        
        # Pontua senten√ßas por import√¢ncia
        sentence_scores = []
        for sentence in sentences:
            score = 0
            sentence_lower = sentence.lower()
            
            # Palavras-chave importantes
            for keyword in self.topic_keywords:
                if keyword in sentence_lower:
                    score += 1
            
            # Verbos de a√ß√£o/decis√£o
            action_words = ['decidiu', 'definiu', 'discutiu', 'apresentou', 'prop√¥s']
            for word in action_words:
                if word in sentence_lower:
                    score += 2
            
            sentence_scores.append((sentence, score))
        
        # Seleciona as melhores
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        top_sentences = [s[0] for s in sentence_scores[:3]]
        
        return '. '.join(top_sentences) + '.'
    
    def _calculate_overall_confidence(
        self, 
        participants: List[ParticipantInfo], 
        topics: List[TopicInfo], 
        actions: List[ActionItem], 
        decisions: List[KeyDecision]
    ) -> float:
        """Calcula confian√ßa geral da an√°lise."""
        confidences = []
        
        # Confian√ßa dos participantes
        if participants:
            avg_participant_confidence = sum(p.confidence for p in participants) / len(participants)
            confidences.append(avg_participant_confidence)
        
        # Confian√ßa dos t√≥picos (baseada na import√¢ncia)
        if topics:
            avg_topic_confidence = sum(t.importance for t in topics) / len(topics)
            confidences.append(avg_topic_confidence)
        
        # Confian√ßa das a√ß√µes
        if actions:
            avg_action_confidence = sum(a.confidence for a in actions) / len(actions)
            confidences.append(avg_action_confidence)
        
        # Confian√ßa das decis√µes
        if decisions:
            avg_decision_confidence = sum(d.confidence for d in decisions) / len(decisions)
            confidences.append(avg_decision_confidence)
        
        if confidences:
            return sum(confidences) / len(confidences)
        else:
            return 0.5  # Confian√ßa padr√£o se n√£o h√° dados


# Inst√¢ncia global do servi√ßo
meeting_analysis_service = MeetingAnalysisService() 